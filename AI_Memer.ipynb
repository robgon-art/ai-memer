{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-Memer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOku29JJurVgrbT37pz5IM4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/ai-memer/blob/main/AI_Memer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19MukmnEaKFH"
      },
      "source": [
        "![AI-memer Output](https://raw.githubusercontent.com/robgon-art/ai-memer/main/images/AI-Memer%20Cover%202.jpg)\n",
        "# AI-Memer: Using Machine Learning to Create Funny Memes \n",
        "### How to create new memes using images from Wikimedia Commons and OpenImages with captions generated automatically by GPT-3 and GPT-Neo\n",
        "By Robert. A Gonsalves\n",
        "\n",
        "You can read my article about this project on Medium.\n",
        "\n",
        "The source code and generated memes are released under the CC BY-SA license.\n",
        "\n",
        "![CC BY-NC-SA](https://licensebuttons.net/l/by-sa/4.0/88x31.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3y5aLIRDE-n"
      },
      "source": [
        "# **Initalize the System**\n",
        "Hit the Run Cell (play) button to intialize the system. Note that it takes about ten minutes to set up all the components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK2pwVCizWGv",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "!wget https://raw.githubusercontent.com/openimages/dataset/master/downloader.py\n",
        "!pip install boto3 > /dev/null 2>&1\n",
        "!gdown --id 1TS5K0BGk5ruCF-bc6yeMSAEb5z8Oi_st\n",
        "!gdown --id 1-2ForMsp58l6DVAeUqEvW0N24-YITf5o\n",
        "\n",
        "import numpy as np\n",
        "text_features16 = np.load(\"ai-memer_embeddings16.npy\")\n",
        "print(text_features16.shape)\n",
        "import pickle\n",
        "annotations = pickle.load(open(\"ai-memer_annotations.pkl\", \"rb\"))\n",
        "print(annotations[520000])\n",
        "\n",
        "!pip install wikipedia\n",
        "\n",
        "import requests\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def download_file(url, path):\n",
        "  filename = url.split(\"/\")[-1]\n",
        "  file_path = os.path.join(path, filename)\n",
        "  r = requests.get(url, stream=True)\n",
        "  if r.status_code == 200:\n",
        "    r.raw.decode_content = True\n",
        "    with open(file_path,'wb') as f:\n",
        "      shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "def get_first_words(text, num):\n",
        "  parts = text.split()\n",
        "  result = \"\"\n",
        "  for p in parts[:num]:\n",
        "    result += p + \" \"\n",
        "  return result.strip()\n",
        "\n",
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "def get_file_info(filename):\n",
        "  url = \"https://magnus-toolserver.toolforge.org/commonsapi.php?image=\" + filename\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "  author_info = soup.find(\"author\")\n",
        "  if author_info is None:\n",
        "    author = \"N/A\"\n",
        "  else:\n",
        "    author = author_info.get_text()\n",
        "    author = BeautifulSoup(author, 'html.parser').get_text()\n",
        "  author = get_first_words(author, 3)\n",
        "\n",
        "  license_info = soup.find(\"license\")\n",
        "  if license_info is None:\n",
        "    license = \"Fair Use\"\n",
        "  else:\n",
        "    license = license_info.get_text()\n",
        "    license = BeautifulSoup(license, 'html.parser').get_text()\n",
        "  license = get_first_words(license, 3)\n",
        "\n",
        "  r = soup.find(\"response\")\n",
        "  desc = r.find(\"description\", recursive=False)\n",
        "  description = BeautifulSoup(desc.get_text(), 'html.parser').get_text()\n",
        "  description = description.replace(\"\\n\", \" \").strip()\n",
        "  if \"English:\" in description:\n",
        "    start = description.find(\"English:\")\n",
        "    description = description[start+8:].strip()\n",
        "    if \":\" in description:\n",
        "      stop = description.find(\":\")\n",
        "      while len(description) > 0 and description[stop] is not \" \":\n",
        "        stop -=1\n",
        "      description = description[0:stop].strip()\n",
        "  description = get_first_words(description, 30)\n",
        "  if len(description) == 0:\n",
        "    filename = url[61:]\n",
        "    description = filename[:-4].replace(\"_\", \" \")\n",
        "\n",
        "  return author, license, description\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "!wget https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt -O model.pt\n",
        "import torch\n",
        "model = torch.jit.load(\"model.pt\").cuda().eval()\n",
        "input_resolution = model.input_resolution.item()\n",
        "context_length = model.context_length.item()\n",
        "vocab_size = model.vocab_size.item()\n",
        "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz\n",
        "from functools import lru_cache\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values())\n",
        "        vocab = vocab + [v+'</w>' for v in vocab]\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge))\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text\n",
        "  \n",
        "!pip install openai\n",
        "!pip install ftfy\n",
        "\n",
        "import ftfy\n",
        "import gzip\n",
        "import regex as re\n",
        "import html\n",
        "tokenizer = SimpleTokenizer()\n",
        "def get_text_features(sentence):\n",
        "  encoded_sentence = tokenizer.encode(sentence)\n",
        "  # print(len(encoded_sentence), len(sentence))\n",
        "  text_tokens = [encoded_sentence[:model.context_length]]\n",
        "  text_input = torch.zeros(len(text_tokens), model.context_length, dtype=torch.long)\n",
        "  for i, tokens in enumerate(text_tokens):\n",
        "    text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
        "  text_input = text_input.cuda()\n",
        "  with torch.no_grad():\n",
        "    text_features = model.encode_text(text_input).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "  return text_features\n",
        "\n",
        "def get_top_N_semantic_similarity(similarity_list, N):\n",
        "  results = zip(range(len(similarity_list)), similarity_list)\n",
        "  results = sorted(results, key=lambda x: x[1],reverse = True)\n",
        "  scores = []\n",
        "  indices = []\n",
        "  for index,score in results[:N]:\n",
        "    scores.append(score)\n",
        "    indices.append(index)\n",
        "  return scores, indices\n",
        "\n",
        "!mkdir open_images\n",
        "!mkdir wiki_images\n",
        "\n",
        "!wget https://github.com/lipsumar/meme-caption/raw/master/impact.ttf\n",
        "\n",
        "\n",
        "def drawTextWithOutline(text, x, y):\n",
        "    draw.text((x-2, y-2), text,(0,0,0),font=font)\n",
        "    draw.text((x+2, y-2), text,(0,0,0),font=font)\n",
        "    draw.text((x+2, y+2), text,(0,0,0),font=font)\n",
        "    draw.text((x-2, y+2), text,(0,0,0),font=font)\n",
        "    draw.text((x, y), text, (255,255,255), font=font)\n",
        "    return\n",
        "\n",
        "def drawText(text, pos):\n",
        "    text = text.upper()\n",
        "    w, h = draw.textsize(text, font) # measure the size the text will take\n",
        "\n",
        "    lineCount = 1\n",
        "    if w > img.width:\n",
        "        lineCount = int(round((w / img.width) + 1))\n",
        "\n",
        "    lines = []\n",
        "    if lineCount > 1:\n",
        "\n",
        "        lastCut = 0\n",
        "        isLast = False\n",
        "        for i in range(0,lineCount):\n",
        "            if lastCut == 0:\n",
        "                cut = int((len(text) / lineCount) * i + 0.5)\n",
        "            else:\n",
        "                cut = lastCut\n",
        "\n",
        "            if i < lineCount-1:\n",
        "                nextCut = int((len(text) / lineCount) * (i+1) + 0.5)\n",
        "            else:\n",
        "                nextCut = len(text)\n",
        "                isLast = True\n",
        "\n",
        "            # make sure we don't cut words in half\n",
        "            if not (nextCut == len(text) or text[nextCut] == \" \"):\n",
        "                while text[nextCut] != \" \":\n",
        "                    nextCut += 1\n",
        "\n",
        "            line = text[cut:nextCut].strip()\n",
        "\n",
        "            # is line still fitting ?\n",
        "            w, h = draw.textsize(line, font)\n",
        "            if not isLast and w > img.width:\n",
        "                nextCut -= 1\n",
        "                while text[nextCut] != \" \":\n",
        "                    nextCut -= 1\n",
        "\n",
        "            lastCut = nextCut\n",
        "            lines.append(text[cut:nextCut].strip())\n",
        "    else:\n",
        "        lines.append(text)\n",
        "\n",
        "    lastY = -h\n",
        "    if pos == \"bottom\":\n",
        "        lastY = img.height - h * (lineCount+1) - 10\n",
        "\n",
        "    for i in range(0, lineCount):\n",
        "        w, h = draw.textsize(lines[i], font)\n",
        "        x = img.width/2 - w/2\n",
        "        y = lastY + h\n",
        "        drawTextWithOutline(lines[i], x, y)\n",
        "        lastY = y\n",
        "\n",
        "def display_image_in_actual_size(im_path, dpi):\n",
        "  im_data = plt.imread(im_path)\n",
        "  height, width, depth = im_data.shape\n",
        "  figsize = width / float(dpi), height / float(dpi)\n",
        "  fig = plt.figure(figsize=figsize)\n",
        "  ax = fig.add_axes([0, 0, 1, 1])\n",
        "  ax.axis('off')\n",
        "  ax.imshow(im_data)\n",
        "  plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps, ImageFont, ImageDraw\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "generator = pipeline('text-generation', device=0, model='EleutherAI/gpt-neo-2.7B')\n",
        "import openai\n",
        "openai.api_key = \"your api key\"\n",
        "def cleanup_caption(caption):\n",
        "  caption = caption.strip().replace('\"', '')\n",
        "  parts = caption.split()\n",
        "  caption = \" \".join(parts).strip()\n",
        "  if caption.endswith(\".\"):\n",
        "    caption = caption[:-1]\n",
        "  caption = caption.upper()\n",
        "  return caption\n",
        "\n",
        "gpt_neo_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_neo_prompt = \"\"\"Create a funny caption for a meme. \n",
        "\n",
        "Theme: disaster girl\n",
        "Image description: A picture of a girl looking at us as her house burns down\n",
        "Caption: There was a spider. It's gone now.\n",
        "\n",
        "Theme: grumpy cat\n",
        "Image description: A face of a cat who looks unhappy\n",
        "Caption: I don't like Mondays\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_warning_level(content_to_classify):\n",
        "  content_filter_results = openai.Completion.create(\n",
        "        engine=\"content-filter-alpha-c4\",\n",
        "        prompt = \"<|endoftext|>\"+content_to_classify+\"\\n--\\nLabel:\",\n",
        "        temperature=0,\n",
        "        max_tokens=1,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        logprobs=10)\n",
        "  output_label = content_filter_results[\"choices\"][0][\"text\"]\n",
        "  # This is the probability at which we evaluate that a \"2\" is likely real\n",
        "  # vs. should be discarded as a false positive\n",
        "  toxic_threshold = -0.355\n",
        "  if output_label == \"2\":\n",
        "    # If the model returns \"2\", return its confidence in 2 or other output-labels\n",
        "    logprobs = content_filter_results[\"choices\"][0][\"logprobs\"][\"top_logprobs\"][0]\n",
        "    # If the model is not sufficiently confident in \"2\",\n",
        "    # choose the most probable of \"0\" or \"1\"\n",
        "    # Guaranteed to have a confidence for 2 since this was the selected token.\n",
        "    if logprobs[\"2\"] < toxic_threshold:\n",
        "      logprob_0 = logprobs.get(\"0\", None)\n",
        "      logprob_1 = logprobs.get(\"1\", None)\n",
        "      # If both \"0\" and \"1\" have probabilities, set the output label\n",
        "      # to whichever is most probable\n",
        "      if logprob_0 is not None and logprob_1 is not None:\n",
        "        if logprob_0 >= logprob_1:\n",
        "          output_label = \"0\"\n",
        "        else:\n",
        "          output_label = \"1\"\n",
        "      # If only one of them is found, set output label to that one\n",
        "      elif logprob_0 is not None:\n",
        "        output_label = \"0\"\n",
        "      elif logprob_1 is not None:\n",
        "        output_label = \"1\"\n",
        "      # If neither \"0\" or \"1\" are available, stick with \"2\"\n",
        "      # by leaving output_label unchanged.\n",
        "  # if the most probable token is none of \"0\", \"1\", or \"2\"\n",
        "  # this should be set as unsafe\n",
        "  if output_label not in [\"0\", \"1\", \"2\"]:\n",
        "    output_label = \"2\"\n",
        "  return output_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV5kL_iHakbA"
      },
      "source": [
        "# **Search for Background Images**\n",
        "Enter a topic then hit the Run Cell button search and download related images from OpenImages and Wikipedia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jyulIdcTEh6",
        "cellView": "form"
      },
      "source": [
        "find_image = \"cat lying on the floor\" #@param {type:\"string\"}\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "text_query = find_image\n",
        "feature_text = \"<|startoftext|> Image of a \" + text_query + \" <|endoftext|>\"\n",
        "query_features = get_text_features(feature_text)\n",
        "text_similarity = query_features.cpu().numpy() @ text_features16.T\n",
        "text_similarity = text_similarity[0]\n",
        "\n",
        "text_scores, text_indices = get_top_N_semantic_similarity(text_similarity, N=20)\n",
        "\n",
        "f = open(\"images.txt\", \"w\")\n",
        "for i in text_indices:\n",
        "  f.write(annotations[i][0] +\"\\n\")\n",
        "f.close()\n",
        "\n",
        "# get the images from OpenImages\n",
        "print(\"Downloading images from OpenImages\")\n",
        "!python downloader.py images.txt --download_folder=open_images --num_processes=1\n",
        "\n",
        "all_images = []\n",
        "all_image_metadata = []\n",
        "all_files = []\n",
        "for i in range(0, 20):\n",
        "  image_id = annotations[text_indices[i]][0]\n",
        "  description = annotations[text_indices[i]][1]\n",
        "  author = annotations[text_indices[i]][2]\n",
        "  parts = image_id.split(\"/\")\n",
        "  file_path = \"open_images/\" + parts[1] + \".jpg\"\n",
        "  if file_path not in all_files:\n",
        "    metadata = (\"OpenImages\", file_path, description, author, \"CC BY 2.0\")\n",
        "    all_image_metadata.append(metadata)\n",
        "    img = Image.open(file_path)\n",
        "    img = img.convert(mode=\"RGB\")\n",
        "    all_images.append(img)\n",
        "    all_files.append(file_path)\n",
        "\n",
        "# get the images Wikipedia\n",
        "print(\"Downloading images from Wikipedia\")\n",
        "results = wikipedia.search(text_query, results=3)\n",
        "print(results)\n",
        "for r in results:\n",
        "  print(r)\n",
        "  try:\n",
        "    p = wikipedia.page(r)\n",
        "  except:\n",
        "    continue\n",
        "  try:\n",
        "    for pi in p.images[:10]:\n",
        "      if pi.endswith(\".jpg\"):\n",
        "        filename = pi.split(\"/\")[-1]\n",
        "        if \"wikipedia/commons\" in pi:\n",
        "          source = \"Wikipedia Commons\"\n",
        "          author, license, description = get_file_info(filename)\n",
        "        else:\n",
        "          source = \"Wikipedia\"\n",
        "          author = \"N/A\"\n",
        "          license = \"Fair Use\"\n",
        "          description = filename[:-4].replace(\"_\", \" \")\n",
        "        download_file(pi, \"wiki_images\")\n",
        "        file_path = \"wiki_images/\" + filename\n",
        "        if file_path not in all_files:\n",
        "          metadata = (source, file_path, description, author, license)\n",
        "          all_image_metadata.append(metadata)\n",
        "          img = Image.open(file_path)\n",
        "          img = img.convert(mode=\"RGB\")\n",
        "          all_images.append(img)\n",
        "          all_files.append(file_path)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode\n",
        "input_resolution = model.input_resolution.item()\n",
        "image_features = torch.empty((0, 512))\n",
        "\n",
        "preprocess = Compose([\n",
        "    Resize(input_resolution, interpolation=InterpolationMode.BICUBIC),\n",
        "    CenterCrop(input_resolution),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).cuda()\n",
        "image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).cuda()\n",
        "images = [preprocess(im) for im in all_images]\n",
        "image_input = torch.tensor(np.stack(images)).cuda()\n",
        "image_input -= image_mean[:, None, None]\n",
        "image_input /= image_std[:, None, None]\n",
        "with torch.no_grad():\n",
        "  image_features = model.encode_image(image_input).float().cpu()  \n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "image_similarity = query_features.cpu().numpy() @ image_features.numpy().T\n",
        "image_similarity = image_similarity[0]\n",
        "image_scores, image_indices = get_top_N_semantic_similarity(image_similarity, N=10)\n",
        "\n",
        "fig=plt.figure(figsize=(26, 8))\n",
        "columns = 5\n",
        "rows = 2\n",
        "for i in range(1, columns*rows + 1):\n",
        "  file_name = all_image_metadata[image_indices[i-1]][1]\n",
        "  img = Image.open(file_name)\n",
        "  img = img.convert(mode=\"RGB\")\n",
        "  fig.add_subplot(rows, columns, i)\n",
        "  plt.margins(y=10)\n",
        "  plt.imshow(img)\n",
        "  plt.text(0, -30, str(i), fontsize=20)\n",
        "  plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "790Jm9wfEtNT"
      },
      "source": [
        "# **Specify Open.AI API Key (Optional)**\n",
        "\n",
        "If you have an OpenAI account, you can use their **GPT-3 davici** model to generate captions by entering your API Key and running the following cell. Note that this will cost about $0.03 per run.\n",
        "\n",
        "Otherwise you can use **Eleuther GPT-Neo** for free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe7DRqqkFqZW"
      },
      "source": [
        "openai.api_key = \"your api key\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esV_nIBXa7u0"
      },
      "source": [
        "# **Generate Captions**\n",
        "Choose an image then hit the Run Cell button to generate captions for this meme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tue4pNRZYylc",
        "cellView": "form"
      },
      "source": [
        "choose_image = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "fig=plt.figure(figsize=(9, 9))\n",
        "file_name = all_image_metadata[image_indices[choose_image-1]][1]\n",
        "img = Image.open(file_name)\n",
        "img = img.convert(mode=\"RGB\")\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "annotation = all_image_metadata[image_indices[choose_image-1]][2]\n",
        "captions = []\n",
        "print(\"Captions:\")\n",
        "if (openai.api_key == \"your api key\"):\n",
        "  ai_engine = \"EleutherAI GPT-Neo\"\n",
        "  prompt = gpt_neo_prompt + \"Theme: \" + text_query + \"\\nDescription: \" + annotation\n",
        "  prompt += \"\\nCaption:\"\n",
        "  for i in range(10):\n",
        "    results = generator(prompt, do_sample=True, min_length=50, max_length=150, \n",
        "      temperature=0.7, top_p=1.0, pad_token_id=gpt_neo_tokenizer.eos_token_id)\n",
        "    caption = results[0][\"generated_text\"]\n",
        "    caption = caption[len(prompt):].strip()\n",
        "    caption = caption[:caption.find(\"\\n\")]\n",
        "    caption = cleanup_caption(caption)\n",
        "    captions.append(caption)\n",
        "    print(f'{i+1:>2}'+\":\", caption)\n",
        "else:\n",
        "  ai_engine = \"OpenAI GPT-3\"\n",
        "  prompt = \"Create a funny caption for a new meme about \" + text_query + \". The background picture is \" + annotation + \".\\n\"\n",
        "  for i in range(10):\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"davinci-instruct-beta\",\n",
        "      prompt=prompt,\n",
        "      temperature=0.7,\n",
        "      max_tokens=64,\n",
        "      top_p=0.5,\n",
        "      frequency_penalty=0.5,\n",
        "      presence_penalty=0.5,\n",
        "      best_of=1\n",
        "    )\n",
        "    caption = response[\"choices\"][0][\"text\"]\n",
        "    warning_level = get_warning_level(caption)\n",
        "    caption = cleanup_caption(caption)\n",
        "    captions.append(caption)\n",
        "    warning = \"\"\n",
        "    if warning_level == \"1\":\n",
        "      warning = \"  (Warning Level 1: Sensitive Text)\"\n",
        "    elif warning_level == \"2\":\n",
        "      warning = \"  (Warning Level 2: Unsafe Text)\"\n",
        "    print(f'{i+1:>2}'+\":\", caption, warning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcVZyZatbVCQ"
      },
      "source": [
        "# **Format the Caption**\n",
        "Choose the caption and formatting options then hit the Run Cell button to generate the meme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vd_jcYK6uTG",
        "cellView": "form"
      },
      "source": [
        "# from https://blog.lipsumarium.com/caption-memes-in-python/\n",
        "\n",
        "choose_caption = 1 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "font_size= 30 #@param {type:\"slider\", min:20, max:60, step:2}\n",
        "position = \"bottom\" #@param [\"top\", \"bottom\"]\n",
        "# image_id = annotations[text_indices[image_indices[0]]][0]\n",
        "# parts = image_id.split(\"/\")\n",
        "\n",
        "file_name = all_image_metadata[image_indices[choose_image-1]][1]\n",
        "img = Image.open(file_name)\n",
        "w, h = img.size\n",
        "if (w > h):\n",
        "  new_w = 512.0\n",
        "  new_h = round(new_w * h / w)\n",
        "else:\n",
        "  new_h = 512.0\n",
        "  new_w = round(new_h * w / h)\n",
        "img = img.resize((int(new_w+0.5), int(new_h+0.5)))\n",
        "img = img.convert(mode=\"RGB\")\n",
        "draw = ImageDraw.Draw(img)\n",
        "font = ImageFont.truetype(\"impact.ttf\", font_size)\n",
        "drawText(captions[choose_caption-1], position)\n",
        "output_filename = \"AI-Memer_\" + text_query.replace(\" \", \"_\")\n",
        "output_filename += \"_\" + str(choose_image) + \"_\" + str(choose_caption) + \".jpg\"\n",
        "img.save(output_filename)\n",
        "display_image_in_actual_size(output_filename, 72)\n",
        "author = all_image_metadata[image_indices[choose_image-1]][3]\n",
        "print(\"Meme by AI-Memer, Image by \" + author + \", Caption by \" + ai_engine + \", License: CC BY-SA 4.0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8FdZay_fOI"
      },
      "source": [
        "# **Download the Meme**\n",
        "Hit the Run Cell button to download the meme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrgWmFn5uS25"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(output_filename)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}